x-gpu-deploy: &gpu-deploy
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [ gpu ]

services:
  ai-base:
    build: ./base
    image: ai-suite-base:latest
    pull_policy: never

  cloudflare:
    build:
      context: ./cloudflare
      args:
        TUNNEL_NAME: ${TUNNEL_NAME}
        DOMAIN_NAME: ${DOMAIN_NAME}
    volumes:
      - ./cloudflare/credentials.json:/cloudflare/credentials.json
      - ./cloudflare/cert.pem:/cloudflare/cert.pem
    depends_on:
      - open-webui

  comfyui:
    build: ./comfyui
    <<: *gpu-deploy
    ports:
      - "127.0.0.1:8188:8188"
    volumes:
      - comfyui-models:/comfyui/models
      - comfyui-output:/comfyui/output
      - comfyui-custom_nodes:/comfyui/custom_nodes
    depends_on:
      - ai-base

  ollama:
    build: ./ollama
    <<: *gpu-deploy
    volumes:
      - ollama-models:/root/.ollama
    depends_on:
      - ai-base
    environment:
      - OLLAMA_HOST=0.0.0.0:11434

  open-webui:
    build: ./open-webui
    env_file:
      - open-webui/config.env
    environment:
      - WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}
    <<: *gpu-deploy
    ports:
      - "0.0.0.0:8080:8080"
    volumes:
      - open-webui:/app/backend/data
      - open-webui-cache:/root/.cache
    depends_on:
      - ai-base

  stt:
    build: ./stt
    <<: *gpu-deploy
    ports:
      - "0.0.0.0:5001:5001"
    volumes:
      - stt-models:/root/.cache/huggingface/hub
    depends_on:
      - ai-base

  tts:
    build: ./tts
    <<: *gpu-deploy
    ports:
      - "0.0.0.0:5000:5000"
    volumes:
      - tts-models:/root/.cache/huggingface/hub
    depends_on:
      - ai-base

volumes:
  comfyui-models:
  comfyui-output:
  comfyui-custom_nodes:
  ollama-models:
  tts-models:
  stt-models:
  open-webui:
  open-webui-cache:
